{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4685806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils.load_utils import prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a81f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train -------\n",
      "Read 45533 sentence pairs\n",
      "Counting words\n",
      "Counted words:\n",
      "In vocabulary: 22970 words\n"
     ]
    }
   ],
   "source": [
    "voc, train_pairs, vector = prepare_data('train', 'glove.42B.300d/glove.42B.300d.txt', small=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07711e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_pairs)):\n",
    "    if train_pairs[i][0] == ' ':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "168a1745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['how long will it take to get the results ?',\n",
       "  \"we will send your doctor the results and he will contact you . eou ' EOS\"],\n",
       " [\"'hello . i need to disconnect my phone please .\",\n",
       "  'all right . where do you live sir ? EOS'],\n",
       " ['at 345 lincoln avenue . oklahoma city .',\n",
       "  'very well . why do you want to disconnect your phone sir ? EOS'],\n",
       " ['i m moving to a new home .', 'o . k . may i have your name please ? EOS'],\n",
       " ['john smith .',\n",
       "  'thank you . mr . smith . what s your telephone number ? EOS'],\n",
       " ['555 7658', 'thank you . where should i send your final phone bill ? EOS']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs[22645:22651]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Voc:\n",
    "    \n",
    "    def __init__(self, name, word2index):\n",
    "        self.name = name\n",
    "        # Create dict of word: 1 (count) for the words in the GloVe vocabulary\n",
    "        self.word_count = {word: 1 for word in word2index.keys()}\n",
    "        # Import the word: index created from load glove embbedding\n",
    "        self.word2index = word2index\n",
    "        self.n_words = len(word2index.keys())\n",
    "        # Reverse index and words \n",
    "        self.index2word = {v: k for k, v in word2index.items()}\n",
    "        \n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word_count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word_count[word] += 1\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            self.add_word(word)\n",
    "\n",
    "\n",
    "\n",
    "def load_glove(file_path, small=True):\n",
    "    idx = 4\n",
    "    vectors = {}\n",
    "    word2idx = {}\n",
    "    with open(file_path, encoding='utf8') as lines:\n",
    "        for line in lines:\n",
    "            # Load only 10000 words if small is called\n",
    "            if small and idx > 10000:\n",
    "                break\n",
    "            # Split the line at the spaces and create a list where first is word and next is the word embedding vectors\n",
    "            line = line.split()\n",
    "            # Assign dict key to the word in the line and value an index \n",
    "            word2idx[line[0].lower()] = idx\n",
    "            # Assign dict key to the word in the line and value a numpay array of the word (embedding from GloVe) \n",
    "            vectors[line[0].lower()] = np.array(list(line[1:]), dtype='float')\n",
    "            idx += 1\n",
    "            embed_dim = len(list(line[1:]))\n",
    "    vectors[0] = np.random.normal(scale=0.6, size=(embed_dim, ))\n",
    "    vectors[1] = np.random.normal(scale=0.6, size=(embed_dim, ))\n",
    "    vectors[2] = np.random.normal(scale=0.6, size=(embed_dim, ))\n",
    "    vectors[3] = np.random.normal(scale=0.6, size=(embed_dim, ))\n",
    "    word2idx['PAD'] = 0\n",
    "    word2idx['SOS'] = 1\n",
    "    word2idx['EOS'] = 2\n",
    "    word2idx['UNK'] = 3\n",
    "    return vectors, word2idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def UnicodeToASCII(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def sentence_cleaning(sentence):\n",
    "    # Transforms to ASCII, lower case and strip blank spaces\n",
    "    sentence = UnicodeToASCII(sentence)\n",
    "    # Get rid of double puntuation\n",
    "    sentence = re.sub(r\"([.!?]+)\\1\", r\"\\1\", sentence.lower().strip())\n",
    "    # Get rid of non-letter character\n",
    "    sentence = re.sub(r\"[^a-zA-Z.!?']+\", r\" \", sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def load_file(name):\n",
    "    lines = open(f'data/{name}/dialogues_{name}.txt', encoding='utf-8').read().strip().split('\\n')\n",
    "    return lines\n",
    "\n",
    "\n",
    "def Read_data(dataset,  glove_file_path, small):\n",
    "    \n",
    "    print(f'Reading {dataset} -------')\n",
    "    # Load one of the three datasets train, test or validation and return a list of all the lines\n",
    "    lines = load_file(dataset)\n",
    "    # Split each line into sentence and create a list of list\n",
    "    list_sentences = [[sentence for sentence in line.split('__eou__')] for line in lines]\n",
    "    # Assumes odd sentences being the source aka question and even sentences the target aka answer, still in a list of list format\n",
    "    source_sentences_list = [[source for source in sentence if sentence.index(source)%2 == 0] for sentence in list_sentences]\n",
    "    target_sentences_list = [[source for source in sentence if sentence.index(source)%2 != 0] for sentence in list_sentences]\n",
    "\n",
    "    for sentence_list in source_sentences_list:\n",
    "        try:\n",
    "            sentence_list.remove('')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            sentence_list.remove(' ')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Flattens the list to have all the questions in one list\n",
    "    source_sentences = [sentence for row in source_sentences_list for sentence in row]\n",
    "    # Flattens the list to have all the answers in one list\n",
    "    target_sentences = [sentence for row in target_sentences_list for sentence in row]\n",
    "    # Creates a pair of question-answer as a list of list\n",
    "    pairs = [[sentence_cleaning(question), sentence_cleaning(answer)] for question, answer in zip(source_sentences, target_sentences)]\n",
    "    # Pad empty sentences\n",
    "    #pairs = [['EMPTY', line[1]] if line[0] == '' else line for line in pairs]\n",
    "    pairs = [[line[0], 'EMPTY'] if line[1] == '' else line for line in pairs]\n",
    "    # Pad spaces\n",
    "    #pairs = [['EMPTY', line[1]] if line[0] == ' ' else line for line in pairs]\n",
    "    pairs = [[line[0], 'EMPTY'] if line[1] == ' ' else line for line in pairs]\n",
    "    # Load GloVe vectors\n",
    "    glove_vectors, glove_word2idx = load_glove(glove_file_path, small)\n",
    "    # Initialize the classes questions and answers to assign indexes and count the words\n",
    "    vocabulary = Voc('vocabulary', glove_word2idx)\n",
    "\n",
    "    return vocabulary, pairs, glove_vectors\n",
    "\n",
    "\n",
    "def prepare_data(dataset, glove_file_path, small=True):\n",
    "    voc, pairs, word_vector = Read_data(dataset, glove_file_path, small)\n",
    "    # Adding EOS in answers\n",
    "    pairs = [[line[0], line[1]+' EOS'] for line in pairs]\n",
    "    print(f'Read {len(pairs)} sentence pairs')\n",
    "    print('Counting words')\n",
    "    for pair in pairs:\n",
    "        voc.add_sentence(pair[0])\n",
    "        voc.add_sentence(pair[1])\n",
    "    \n",
    "    print('Counted words:')\n",
    "    print(f'In {voc.name}: {voc.n_words} words')\n",
    "    \n",
    "    return voc, pairs, word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = load_file('train')\n",
    "# Split each line into sentence and create a list of list\n",
    "list_sentences = [[sentence for sentence in line.split('__eou__')] for line in lines]\n",
    "# Assumes odd sentences being the source aka question and even sentences the target aka answer, still in a list of list format\n",
    "source_sentences_list = [[source for source in sentence if sentence.index(source)%2 == 0] for sentence in list_sentences]\n",
    "target_sentences_list = [[source for source in sentence if sentence.index(source)%2 != 0] for sentence in list_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf204bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence_list in source_sentences_list:\n",
    "        try:\n",
    "            sentence_list.remove('')\n",
    "            sentence_list.remove(' ')\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(target_sentences_list)):\n",
    "    for j in range(len(target_sentences_list[i])):\n",
    "        if target_sentences_list[i][j] == ' ':\n",
    "            print(target_sentences_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db482d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences = [sentence for row in source_sentences_list for sentence in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentences = [sentence for row in target_sentences_list for sentence in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa96bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentences[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1063d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = zip(source_sentences, target_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a338b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cee56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[question, answer] for question, answer in zip(source_sentences, target_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485edef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = load_file('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e54f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences = [[sentence for sentence in line.split('\\n')] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences = [[sentence for sentence in str(line).strip('][\"').split(' __eou__ ')] for line in list_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b924319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_sentences = [[sentence for sentence in line.split(' __eou__')] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in list_sentences:\n",
    "    if len(sentence)%2 != 0:\n",
    "        sentence.append('EMPTY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a377233",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_sentences[1060])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49061188",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences[1060]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce3bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences_list = [[source for source in sentence if sentence.index(source)%2 == 0] for sentence in list_sentences]\n",
    "target_sentences_list = [[source for source in sentence if sentence.index(source)%2 != 0] for sentence in list_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences_list = []\n",
    "for idx in range(len(list_sentences)):\n",
    "    source_list = []\n",
    "    for u_idx in range(len(list_sentences[idx])):\n",
    "        if u_idx%2 == 0:\n",
    "            source_list.append(list_sentences[idx][u_idx])\n",
    "        \n",
    "    source_sentences_list.append(source_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentences_list = []\n",
    "for idx in range(len(list_sentences)):\n",
    "    target_list = []\n",
    "    for u_idx in range(len(list_sentences[idx])):\n",
    "        if u_idx%2 != 0:\n",
    "            target_list.append(list_sentences[idx][u_idx])\n",
    "        \n",
    "    target_sentences_list.append(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d455bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences_list[1060]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd4955",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentences_list[1060]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207bb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences_list[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca03844",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentences_list[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee864e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(source_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95999d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71975c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.Series(target_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d131f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.concat([data, data2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1622f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addf6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3['source_len'] = data3[0].apply(lambda x: len(x))\n",
    "data3['target_len'] = data3[1].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82833e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.iloc[11116, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9cf016",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.iloc[12, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e370b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.iloc[3190, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a546558",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.iloc[1060, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3[data3['source_len'] != data3['target_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f517359",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8d4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences = [sentence for row in source_sentences_list for sentence in row]\n",
    "source_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentences_list = [[source for source in sentence if sentence.index(source)%2 != 0] for sentence in list_sentences]\n",
    "target_sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dcbe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentences = [sentence for row in target_sentences_list for sentence in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0455a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[question, answer] for question, answer in zip(source_sentences, target_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080668f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
